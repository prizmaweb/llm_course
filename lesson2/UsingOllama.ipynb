{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {
    "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458"
   },
   "source": [
    "# USING OLLAMA\n",
    "\n",
    "---\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {
    "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {
    "id": "29ddd15d-a3c5-4f4e-a678-873f56162724"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {
    "id": "dac0a679-599c-441f-9bf2-ddc73d35b940"
   },
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Descrble what Akamai does\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {
    "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47"
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {
    "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
    "outputId": "4b069a3b-2232-4062-b168-d1f3e2b23f19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {
    "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
    "outputId": "47c18efd-2ada-41a5-952b-c07999eb0c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the translation:\n",
      "\n",
      "```\n",
      "あらためて、ありがとうございました。\n",
      "```\n",
      "\n",
      "Or, in a more casual tone:\n",
      "\n",
      "```\n",
      "すごくありがとう\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
   "metadata": {
    "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe"
   },
   "source": [
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {
    "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
    "outputId": "5eddaa05-afd2-4575-9a35-2ca22d15583e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the translation:\n",
      "\n",
      "「ありがとう」 (Arigatou)\n",
      "\n",
      "Or, in a more polite and formal tone:\n",
      "\n",
      "「ありがとうございます」 (Arigatou gozaimasu)\n",
      "\n",
      "The latter is used when expressing gratitude or appreciation for something. The former is a more casual way of saying \"thank you\".\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {
    "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d"
   },
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {
    "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
    "outputId": "1fa75247-f226-4053-a2b0-6fa8feab16df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akamai Technologies is a leading provider of cloud-based service solutions for accelerating and securing online content delivery. Here's an overview of what they do:\n",
      "\n",
      "**Key Services:**\n",
      "\n",
      "1. **Content Delivery Network (CDN)**: Akamai's Content Delivery Network ( CDN) is a global network of servers strategically located across the world. The CDN acts as a proxy between users' devices and the origin server, caching frequently accessed content at edge locations closer to users. This reduces latency, improves performance, and ensures high availability.\n",
      "2. **Security Services**: Akamai offers advanced security solutions, including web applicationFirewall (WAF), bot mitigation, and DDoS protection. Their services help protect websites, web applications, and APIs from cyber threats like malware, SQL injection, and cross-site scripting (XSS) attacks.\n",
      "3. **CDN + Security Integration**: Akamai's full-service CDN+Security allows organizations to combine their CDNs with security features, providing an integrated solution for accelerating online content while protecting it from cyber threats.\n",
      "\n",
      "**Additional Features:**\n",
      "\n",
      "* High-performance delivery of multimedia and live streaming content\n",
      "* Support for real-time applications like video conferencing and social media platforms\n",
      "* Cloud-based storage solutions for backup and disaster recovery purposes (Akamai's OpenEdge)\n",
      "* Integration with other cloud services like AWS, Azure, Google Cloud, and Amazon Web Services\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "By using Akamai's services, organizations can:\n",
      "\n",
      "* Enhance user experience through faster page loads and improved performance\n",
      "* Reduce latency and improve availability for online applications\n",
      "* Increase security against cyber threats and protect sensitive data\n",
      "* Simplify management of content delivery networks and security controls\n",
      "\n",
      "In summary, Akamai helps organizations deliver high-performance, secure, and scalable online experiences across various industries.\n"
     ]
    }
   ],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44",
   "metadata": {
    "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44"
   },
   "source": [
    "## Are you confused about why that works?\n",
    "\n",
    "It seems strange, right? We just used OpenAI code to call Ollama?? What's going on?!\n",
    "\n",
    "Here's the scoop:\n",
    "\n",
    "The python class `OpenAI` is simply code written by OpenAI engineers that makes calls over the internet to an endpoint.  \n",
    "\n",
    "When you call `openai.chat.completions.create()`, this python code just makes a web request to the following url: \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "Code like this is known as a \"client library\" - it's just wrapper code that runs on your machine to make web requests. The actual power of GPT is running on OpenAI's cloud behind this API, not on your computer!\n",
    "\n",
    "OpenAI was so popular, that lots of other AI providers provided identical web endpoints, so you could use the same approach.\n",
    "\n",
    "So Ollama has an endpoint running on your local box at http://localhost:11434/v1/chat/completions  \n",
    "And in week 2 we'll discover that lots of other providers do this too, including Gemini and DeepSeek.\n",
    "\n",
    "And then the team at OpenAI had a great idea: they can extend their client library so you can specify a different 'base url', and use their library to call any compatible API.\n",
    "\n",
    "That's it!\n",
    "\n",
    "So when you say: `ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')`  \n",
    "Then this will make the same endpoint calls, but to Ollama instead of OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
   "metadata": {
    "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90"
   },
   "source": [
    "## Also trying the amazing reasoning model DeepSeek\n",
    "\n",
    "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
    "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
    "\n",
    "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d",
   "metadata": {
    "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d"
   },
   "outputs": [],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
   "metadata": {
    "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
    "outputId": "7c33db7a-62d8-4b08-dcb1-769c3f143596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to explain the key concepts behind Large Language Models (LLMs), specifically focusing on what neural networks are, attention mechanisms in transformers, and maybe something about how transformers themselves work. Let me break this down step by step.\n",
      "\n",
      "First, understanding what a neural network is. Neural networks are computational models inspired by the structure of the human brain. They consist of layers, nodes (or neurons) connected together with weights that represent connections or dependencies. So these models process information through layers where each layer transforms the inputs into representations that can be used for the next layer. Think of it as a way to model complex patterns in data.\n",
      "\n",
      "Next, attention mechanisms in transformers. I remember transformers are known for their efficiency and effectiveness in NLP tasks. The key part here is how they manage the flow of information. The mechanism called \" attended self-attention\" allows each token in a sequence (like words or genes) to attend to a subset of all tokens that come after it. This creates local context, helping the model understand which parts of the input are most relevant when making decisions.\n",
      "\n",
      "Then, what exactly is a transformer? Transformers are a type of neural network architecture designed by researchers like Vaswani and others. Unlike convolutional architectures that focus on spatial features, transformers process data in permutation-invariant vector spaces, meaning their representations don't depend on the order of input tokens. Each token attends to previous and future tokens, which is a unique feature compared to most other models.\n",
      "\n",
      "Putting it all together: LLMs leverage neural networks with attention mechanisms, particularly the transformer's ability to learn local context through self-attention. The architecture allows each model or \"agent\" in an NLP task to attend only locally to its neighbors and past experiences, enabling them to adapt and solve complex problems based on contextual information.\n",
      "\n",
      "I think that covers all the main points without getting too detailed. Maybe add a bit of summary at the end for clarity.\n",
      "</think>\n",
      "\n",
      "Large Language Models (LLMs) are sophisticated AI systems designed to process and generate human-level text or speech. At their core, they employ neural networks with attention mechanisms:\n",
      "\n",
      "1. **Neural Networks**: These are computational models composed of layers of nodes, interconnected through weighted connections. Each layer transforms input into representations that can be used for the next layer, allowing them to model complex patterns in data.\n",
      "\n",
      "2. **Attention Mechanisms**: Specifically, self-attention within transformer architectures enable models to focus on local context. Each token attends only to previous and future tokens, creating hierarchical representations that capture local relationships.\n",
      "\n",
      "3. **Transformer Architecture**: Unlike traditional neural networks with fixed layers, transformers use a permutation-invariant vector space. Each token attends globally but locally, processing input through self-attention, enabling them to learn context and adapt.\n",
      "\n",
      "In summary, LLMs leverage neural networks with attention mechanisms, particularly the transformer's ability to attend only local information, allowing them to process data effectively for tasks like text generation and translation.\n"
     ]
    }
   ],
   "source": [
    "# This may take a few minutes to run! You should then see a fascinating \"thinking\" trace inside <think> tags, followed by some decent definitions\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
